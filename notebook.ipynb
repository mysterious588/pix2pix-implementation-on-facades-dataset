{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# prepare the data\n* trainA, trainB will be paired as X, y\n* testA, testB will also be paired as X, y","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\n\n# Define the data directory\ndata_dir = '/kaggle/input/facades-dataset'\n\n# Define the image size and batch size\nIMG_SIZE = 256\nBATCH_SIZE = 1\n\n# Get the file paths for the train and test data\ntrain_a_paths = tf.data.Dataset.list_files(os.path.join(data_dir, 'trainA', '*.jpg'))\ntrain_b_paths = tf.data.Dataset.list_files(os.path.join(data_dir, 'trainB', '*.jpg'))\ntest_a_paths = tf.data.Dataset.list_files(os.path.join(data_dir, 'testA', '*.jpg'))\ntest_b_paths = tf.data.Dataset.list_files(os.path.join(data_dir, 'testB', '*.jpg'))\n\n# Combine the train and test data into pairs\ntrain_pairs = tf.data.Dataset.zip((train_b_paths, train_a_paths))\ntest_pairs = tf.data.Dataset.zip((test_b_paths, test_a_paths))\n\n# Define a function to load and preprocess the images\ndef load_image(image_file):\n    # Read the image file\n    image = tf.io.read_file(image_file)\n    # Decode the image to a tensor\n    image = tf.image.decode_jpeg(image)\n    # Resize the image\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n    # Normalize the image to [-1, 1]\n    image = (image / 127.5) - 1\n    return image\n\n# Load and preprocess the training images\ntrain_dataset = train_pairs.map(lambda x, y: (load_image(x), load_image(y)))\n\n# Load and preprocess the testing images\ntest_dataset = test_pairs.map(lambda x, y: (load_image(x), load_image(y)))\n\n# Shuffle and batch the training data\ntrain_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE)\n\n# Print the number of training and testing examples\nprint(\"Number of training examples: \", tf.data.experimental.cardinality(train_dataset).numpy())\nprint(\"Number of testing examples: \", tf.data.experimental.cardinality(test_dataset).numpy())","metadata":{"execution":{"iopub.status.busy":"2023-05-28T07:24:31.836350Z","iopub.execute_input":"2023-05-28T07:24:31.836657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# define generator\n","metadata":{}},{"cell_type":"code","source":"def generator():\n    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n\n    # Encoder\n    down1 = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same', activation='relu')(inputs)\n    down2 = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same', activation='relu', \n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(down1)\n    down3 = tf.keras.layers.Conv2D(256, 4, strides=2, padding='same', activation='relu', \n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(down2)\n    down4 = tf.keras.layers.Conv2D(512, 4, strides=2, padding='same', activation='relu', \n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(down3)\n    down5 = tf.keras.layers.Conv2D(512, 4, strides=2, padding='same', activation='relu', \n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(down4)\n    down6 = tf.keras.layers.Conv2D(512, 4, strides=2, padding='same', activation='relu', \n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(down5)\n    down7 = tf.keras.layers.Conv2D(512, 4, strides=2, padding='same', activation='relu', \n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(down6)\n    down8 = tf.keras.layers.Conv2D(512, 4, strides=2, padding='same', activation='relu', \n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(down7)\n\n    # Decoder\n    up1 = tf.keras.layers.Conv2DTranspose(512, 4, strides=2, padding='same', activation='relu',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(down8)\n    up1 = tf.keras.layers.Concatenate()([up1, down7])\n    up2 = tf.keras.layers.Conv2DTranspose(512, 4, strides=2, padding='same', activation='relu',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(up1)\n    up2 = tf.keras.layers.Concatenate()([up2, down6])\n    up3 = tf.keras.layers.Conv2DTranspose(512, 4, strides=2, padding='same', activation='relu',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(up2)\n    up3 = tf.keras.layers.Concatenate()([up3, down5])\n    up4 = tf.keras.layers.Conv2DTranspose(512, 4, strides=2, padding='same', activation='relu',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(up3)\n    up4 = tf.keras.layers.Concatenate()([up4, down4])\n    up5 = tf.keras.layers.Conv2DTranspose(256, 4, strides=2, padding='same', activation='relu',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(up4)\n    up5 = tf.keras.layers.Concatenate()([up5, down3])\n    up6 = tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same', activation='relu',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(up5)\n    up6 = tf.keras.layers.Concatenate()([up6, down2])\n    up7 = tf.keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(up6)\n    up7 = tf.keras.layers.Concatenate()([up7, down1])\n    outputs = tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(up7)\n\n    return tf.keras.Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:14:02.327118Z","iopub.execute_input":"2023-04-17T17:14:02.327468Z","iopub.status.idle":"2023-04-17T17:14:02.347570Z","shell.execute_reply.started":"2023-04-17T17:14:02.327436Z","shell.execute_reply":"2023-04-17T17:14:02.346458Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# define discriminator","metadata":{}},{"cell_type":"code","source":"def discriminator():\n    inputs = tf.keras.layers.Input(shape=[256, 256, 6])\n\n    # Convolutional layers\n    conv1 = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same', activation='LeakyReLU')(inputs)\n    conv2 = tf.keras.layers.Conv2D(128, 4, strides=2, padding='same', activation='LeakyReLU',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(conv1)\n    conv3 = tf.keras.layers.Conv2D(256, 4, strides=2, padding='same', activation='LeakyReLU',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(conv2)\n    conv4 = tf.keras.layers.Conv2D(512, 4, strides=1, padding='same', activation='LeakyReLU',\n                                     kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(conv3)\n    outputs = tf.keras.layers.Conv2D(1, 4, strides=1, padding='same', activation='sigmoid',\n                                       kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))(conv4)\n\n    return tf.keras.Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:14:02.881481Z","iopub.execute_input":"2023-04-17T17:14:02.882253Z","iopub.status.idle":"2023-04-17T17:14:02.891906Z","shell.execute_reply.started":"2023-04-17T17:14:02.882213Z","shell.execute_reply":"2023-04-17T17:14:02.890547Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# define BinaryCrossentropy losses","metadata":{}},{"cell_type":"code","source":"def generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(disc_generated_output), disc_generated_output)\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n    return total_gen_loss\n\ndef discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(disc_real_output), disc_real_output)\n    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(disc_generated_output), disc_generated_output)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:14:03.300226Z","iopub.execute_input":"2023-04-17T17:14:03.300854Z","iopub.status.idle":"2023-04-17T17:14:03.308198Z","shell.execute_reply.started":"2023-04-17T17:14:03.300813Z","shell.execute_reply":"2023-04-17T17:14:03.306929Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# define Adam optimizers","metadata":{}},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:14:03.819314Z","iopub.execute_input":"2023-04-17T17:14:03.820483Z","iopub.status.idle":"2023-04-17T17:14:03.839414Z","shell.execute_reply.started":"2023-04-17T17:14:03.820437Z","shell.execute_reply":"2023-04-17T17:14:03.838458Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# define training steps","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n@tf.function\ndef train_step(input_image, target):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(input_image, training=True)\n\n        disc_real_output = discriminator(tf.concat([input_image, target], axis=-1), training=True)\n        disc_generated_output = discriminator(tf.concat([input_image, gen_output], axis=-1), training=True)\n\n        gen_loss = generator_loss(disc_generated_output, gen_output, target)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n\ndef fit(train_ds, epochs, test_ds):\n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch + 1, epochs))\n\n        for input_image, target in train_ds:\n            train_step(input_image, target)\n\n        for example_input, example_target in test_ds.take(1):\n            generate_images(generator, tf.expand_dims(example_input, axis=0), tf.expand_dims(example_target, axis=0), epoch)\n\ndef generate_images(model, test_input, tar, epoch):\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(15, 15))\n    display_list = [test_input[0], tar[0], prediction[0]]\n    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n    for i in range(3):\n        plt.subplot(1, 3, i + 1)\n        plt.title(title[i])\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:14:04.320314Z","iopub.execute_input":"2023-04-17T17:14:04.320977Z","iopub.status.idle":"2023-04-17T17:14:04.333906Z","shell.execute_reply.started":"2023-04-17T17:14:04.320940Z","shell.execute_reply":"2023-04-17T17:14:04.332775Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# do the magic","metadata":{}},{"cell_type":"code","source":"BUFFER_SIZE = 400\nBATCH_SIZE = 32\nLAMBDA = 100\nepochs = 150\n\ngenerator = generator()\ndiscriminator = discriminator()\n\nfit(train_dataset, epochs, test_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}